\section{Численные методы решения задач оптимизации. Метод Ньютона и секущей. Методы покоординатного и градиентного спуска}

Оптимизация — это процесс нахождения минимума или максимума функции при заданных ограничениях (или без них). В задачах прикладной математики часто встречаются функции, которые слишком сложны для аналитического нахождения экстремума, поэтому используют численные методы.

В данной секции мы рассмотрим:
\begin{itemize}
    \item Метод Ньютона;
    \item Метод секущей;
    \item Метод покоординатного спуска;
    \item Метод градиентного спуска.
\end{itemize}

\subsection{Постановка задачи оптимизации}
Пусть дана функция:
\[
f: \mathbb{R}^n \to \mathbb{R}, \quad f(\mathbf{x}) = f(x_1, x_2, \dots, x_n)
\]
Необходимо найти точку $\mathbf{x}^* \in \mathbb{R}^n$, такую что:
\[
f(\mathbf{x}^*) \le f(\mathbf{x}), \quad \forall \mathbf{x} \in \mathbb{R}^n
\]
(для задачи минимизации).

Для поиска минимума часто используют производные:
\begin{itemize}
    \item \textbf{Необходимое условие экстремума:} $\nabla f(\mathbf{x}^*) = 0$
    \item \textbf{Достаточное условие минимума:} матрица Гессе $H_f(\mathbf{x}^*)$ положительно определена.
\end{itemize}

\subsection{Метод Ньютона}
Метод Ньютона — это численный метод, который использует разложение функции в ряд Тейлора второго порядка для приближения к экстремуму.

\subsubsection{Алгоритм в одномерном случае}
Для уравнения $f'(x) = 0$:
\[
x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)}
\]
Здесь:
\begin{itemize}
    \item $f'(x_k)$ — первая производная функции в точке $x_k$;
    \item $f''(x_k)$ — вторая производная.
\end{itemize}

\subsubsection{Многомерный случай}
Для векторной функции:
\[
\mathbf{x}_{k+1} = \mathbf{x}_k - H_f^{-1}(\mathbf{x}_k) \nabla f(\mathbf{x}_k)
\]
где $H_f(\mathbf{x}_k)$ — матрица Гессе.

\subsubsection{Плюсы и минусы}
\begin{itemize}
    \item \textbf{Плюсы:} Быстрая сходимость (обычно квадратичная).
    \item \textbf{Минусы:} Нужно вычислять и инвертировать матрицу Гессе, что дорого для больших $n$.
\end{itemize}

\subsubsection{Графическая иллюстрация}
\begin{center}
\begin{tikzpicture}[scale=1]
\draw[->] (-1,0) -- (4,0) node[right] {$x$};
\draw[->] (0,-1) -- (0,4) node[above] {$f(x)$};
\draw[domain=-0.5:3.5,smooth,variable=\x,blue] plot ({\x},{0.5*(\x-1.5)^2+1});
\draw[dashed, red] (0.5,0) -- (0.5,2) node[above] {$x_0$};
\draw[dashed, red] (1.25,0) -- (1.25,1.05) node[above] {$x_1$};
\end{tikzpicture}
\end{center}

\subsection{Метод секущей}
Метод секущей — это упрощённая версия метода Ньютона, в которой вторую производную заменяют приближением по разностям.

\[
x_{k+1} = x_k - f'(x_k) \cdot \frac{x_k - x_{k-1}}{f'(x_k) - f'(x_{k-1})}
\]

\begin{itemize}
    \item \textbf{Плюс:} Не нужно вычислять вторую производную.
    \item \textbf{Минус:} Скорость сходимости ниже, чем у метода Ньютона.
\end{itemize}

\subsection{Метод покоординатного спуска}
Метод покоординатного спуска оптимизирует функцию, изменяя одну координату за раз, оставляя остальные фиксированными.

\subsubsection{Алгоритм}
\begin{enumerate}
    \item Выбираем начальную точку $\mathbf{x}_0$.
    \item Для каждой координаты $i$ ищем минимум функции по $x_i$ при фиксированных остальных координатах.
    \item Повторяем процесс до сходимости.
\end{enumerate}

\subsubsection{Особенности}
\begin{itemize}
    \item Подходит для задач, где минимизация по одной переменной проста.
    \item Может сходиться медленно, если переменные сильно связаны.
\end{itemize}

\subsection{Метод градиентного спуска}
Метод градиентного спуска — один из самых популярных методов оптимизации.

\subsubsection{Основная идея}
Движемся из текущей точки в направлении, противоположном градиенту функции (т.к. градиент указывает направление наибольшего роста).

\[
\mathbf{x}_{k+1} = \mathbf{x}_k - \alpha \nabla f(\mathbf{x}_k)
\]
где $\alpha > 0$ — шаг обучения.

\subsubsection{Выбор шага}
\begin{itemize}
    \item Слишком большой $\alpha$ — можем «перепрыгнуть» минимум.
    \item Слишком маленький $\alpha$ — сходимость медленная.
\end{itemize}

\subsubsection{Графическая иллюстрация}
\begin{center}
\begin{tikzpicture}[scale=1]
\draw[->] (-2,0) -- (3,0) node[right] {$x$};
\draw[->] (0,-0.5) -- (0,4) node[above] {$f(x)$};
\draw[domain=-1.5:2.5,smooth,variable=\x,blue] plot ({\x},{(\x-1)^2+0.5});
\foreach \x in {-1, -0.2, 0.5, 0.8, 1} {
    \fill[red] (\x,{(\x-1)^2+0.5}) circle (2pt);
}
\end{tikzpicture}
\end{center}

\subsubsection{Варианты}
\begin{itemize}
    \item \textbf{С постоянным шагом}
    \item \textbf{С адаптивным шагом} (например, Adam, RMSprop)
\end{itemize}

\subsection{Заключение}
Выбор метода оптимизации зависит от свойств задачи:
\begin{itemize}
    \item Метод Ньютона — быстрый, но требует вычислений второй производной.
    \item Метод секущей — компромисс, подходит для случаев, когда вторая производная недоступна.
    \item Покоординатный спуск — полезен при раздельной оптимизации переменных.
    \item Градиентный спуск — универсальный, особенно в задачах машинного обучения.
\end{itemize}
